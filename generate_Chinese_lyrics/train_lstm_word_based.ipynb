{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, LSTM, Dense, Input, Dropout, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "SEQ_LENGTH = 3     \n",
    "MAX_NB_WORDS = 10000    \n",
    "EMBEDDING_DIM = 512     \n",
    "EMBEDDING_DIM_2 = 512     \n",
    "EMBEDDING_DIM_3 = 256\n",
    "BATCH_SIZE = 1024   \n",
    "EPOCHS = 100    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutWords(file_name):\n",
    "    with open(file_name, 'r', encoding='utf8') as f:\n",
    "        content = f.read().replace('\\n', '。')   # 使用句号作为句子的结束符\n",
    "        f.close()\n",
    "    return list(content)\n",
    "\n",
    "def mapWords(cut_word_list):\n",
    "    \"\"\"\n",
    "     set word2index and index2word to build dictionary\n",
    "    :param cut_word_list: Character-level token\n",
    "    :return:word2index和index2word， key <=> value\n",
    "    \"\"\"\n",
    "    vocabulary = sorted(list(set(cut_word_list)))\n",
    "    word_to_index = dict((w, i+2) for i, w in enumerate(vocabulary))\n",
    "    word_to_index[\"PAD\"] = 0   # 填补\n",
    "    word_to_index[\"UNK\"] = 1   # unknown\n",
    "    index_to_word = dict((index, word) for word, index in word_to_index.items())\n",
    "\n",
    "    word_to_index_json = json.dumps(word_to_index)\n",
    "    index_to_word_json = json.dumps(index_to_word)\n",
    "    with open('./word_to_index_word.txt', 'w', encoding='utf8') as w:\n",
    "        w.write(word_to_index_json)\n",
    "        w.close()\n",
    "    with open('./index_to_word_word.txt', 'w', encoding='utf8') as w:\n",
    "        w.write(index_to_word_json)\n",
    "        w.close()\n",
    "    # print(\"len of word_to_index::\", len(word_to_index))\n",
    "    # print(\"len of index_to_word::\", len(index_to_word))\n",
    "    return word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTrainData(cut_word_list, word_to_index):\n",
    "    \"\"\"\n",
    "    :return:X_train, X_val, y_train, y_val：training and validation\n",
    "    \"\"\"\n",
    "    # 生成训练数据\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    data_index = []\n",
    "    n_all_words = len(cut_word_list)\n",
    "    for i in range(0, n_all_words - SEQ_LENGTH - 1):\n",
    "        seq_x_y = cut_word_list[i: i+SEQ_LENGTH + 1]   # SEQ_LENGTH Chinese characters correspond to the next (SEQ_LENGTH+1)th Chinese characters\n",
    "        index_x_y = [word_to_index[elem] for elem in seq_x_y]    \n",
    "        data_index.append(index_x_y)\n",
    "    np.random.shuffle(data_index)\n",
    "    for i in range(0, len(data_index)):\n",
    "        X_data.append(data_index[i][:SEQ_LENGTH])\n",
    "        y_data.append(data_index[i][SEQ_LENGTH])\n",
    "\n",
    "    #list => tensor\n",
    "    X = np.reshape(X_data, (len(X_data), SEQ_LENGTH))\n",
    "    y = np_utils.to_categorical(y_data)\n",
    "   \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_lstm(X_train, X_val, y_train, y_val, word_to_index):\n",
    "    \"\"\"\n",
    "    Using Tensorboard as call_back \n",
    "    \"\"\"\n",
    "    input_shape = (SEQ_LENGTH,)\n",
    "    x_train_in = Input(input_shape, dtype='int32', name=\"x_train\")\n",
    "\n",
    "    # word_index存储的是所有vocabulary的映射关系\n",
    "    nb_words = min(MAX_NB_WORDS, len(word_to_index))\n",
    "    embedding_layer = Embedding(nb_words, 256, input_length=SEQ_LENGTH)(x_train_in)\n",
    "    print(\"embedding layer is::\", embedding_layer)\n",
    "    print(\"build model.....\")\n",
    "\n",
    "    # return_sequences=True表示返回的是序列，否则下面的LSTM无法使用，但是如果下一层不是LSTM，则可以不写\n",
    "    lstm_1 = Bidirectional(LSTM(EMBEDDING_DIM, name=\"LSTM_1\", return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(embedding_layer)\n",
    "    #drop_1=Dropout(0.2)(lstm_1)\n",
    "    lstm_2 = Bidirectional(LSTM(EMBEDDING_DIM_2, name=\"LSTM_2\", return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(lstm_1)#(drop_1)\n",
    "    #drop_2=Dropout(0.2)(lstm_2)\n",
    "    lstm_3 = Bidirectional(LSTM(EMBEDDING_DIM_3, name=\"LSTM_3\", dropout=0.2, recurrent_dropout=0.2))(lstm_2)#(drop_2)\n",
    "    #drop_3=Dropout(0.2)(lstm_3)\n",
    "    dense = Dense(nb_words, activation=\"softmax\", name=\"Dense_1\")(lstm_3)#(drop_3)\n",
    "\n",
    "    model = Model(inputs=x_train_in, outputs=dense)\n",
    "    print(model.summary())\n",
    "\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "    print(\"Train....\")\n",
    "\n",
    "    # save tensorboard info\n",
    "    tensorboard = TensorBoard(log_dir='./tensorboard_log/')\n",
    "    # save best model.\n",
    "    checkpoint = ModelCheckpoint(filepath='./model_epoch50_2lstm_1dense_seq50_phrase_based_best.h5',\n",
    "                                 monitor='val_loss', mode='min', save_best_only=True, save_weights_only=False, period=1, verbose=1)\n",
    "    reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5)\n",
    "    callback_list = [tensorboard, checkpoint, reduce]\n",
    "\n",
    "    history_record = model.fit(X_train, y_train,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            validation_data=(X_val, y_val),\n",
    "                            epochs=EPOCHS,\n",
    "                            callbacks=callback_list\n",
    "                             )\n",
    "    #validation_data=(X_val, y_val),\n",
    "    model.save('./model_epoch50_2lstm_1dense_seq50_phrase_based_best.h5')\n",
    "    return history_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotAccuray(history_record):\n",
    "    \"\"\"\n",
    "    plot the accuracy and loss line. \n",
    "    :param history_record:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    accuracy_train = history_record.history[\"acc\"]\n",
    "    accuracy_val= history_record.history[\"val_acc\"]\n",
    "    loss_train = history_record.history[\"loss\"]\n",
    "    loss_val = history_record.history[\"val_loss\"]\n",
    "    epochs = range(len(accuracy_train))\n",
    "    plt.plot(epochs, accuracy_train, 'bo', label='Training accuracy')\n",
    "    plt.plot(epochs, accuracy_val, 'b', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss_train, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, loss_val, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"./train_data/all_5.txt\"\n",
    "cut_word_list = cutWords(file_name)\n",
    "word_to_index, index_to_word = mapWords(cut_word_list)\n",
    "X_train, X_val, y_train, y_val = generateTrainData(cut_word_list, word_to_index)\n",
    "history_record = model_lstm(X_train, X_val, y_train, y_val, word_to_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
