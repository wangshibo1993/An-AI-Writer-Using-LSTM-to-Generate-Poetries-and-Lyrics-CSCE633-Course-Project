{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from snownlp import SnowNLP\n",
    "\n",
    "a = tf.test.is_built_with_cuda()\n",
    "b = tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")     \n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = glob.glob('./dataset/json/poet.*.json')\n",
    "poets = []\n",
    "#print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths:\n",
    "    data = open(path, 'r', encoding='utf-8').read()\n",
    "    data = json.loads(data)\n",
    "    #print(data)\n",
    "    for item in data:\n",
    "        content = ''.join(item['paragraphs'])\n",
    "        if len(content)>= 24 and len(content) <= 32:\n",
    "            content = SnowNLP(content)\n",
    "            content=content.han\n",
    "            if len(content)%4==0:\n",
    "                poets.append('[' + content + ']')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 104147 Chinese ancient poets [欲出未出光辣达，千山万山如火发。须臾走向天上来，逐却残星赶却月。] [书劒催人不暂闲，洛阳羁旅复秦关。容颜岁岁愁边改，乡国时时梦里还。]\n"
     ]
    }
   ],
   "source": [
    "print('We have %d Chinese ancient poets' % len(poets), poets[0], poets[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(poets)\n",
    "num_validation_samples = 8000\n",
    "validation_data = poets[:num_validation_samples]\n",
    "training_data=poets[num_validation_samples:]\n",
    "training_data.sort(key=lambda x: len(x))\n",
    "validation_data.sort(key=lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96147\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[命啸无人啸，含娇何处娇？徘徊花上月，空度可怜宵。]\n"
     ]
    }
   ],
   "source": [
    "print(validation_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 3131174 words\n",
      "We have 7927 different words in training set\n",
      "[('，', 193150), ('。', 192667), ('[', 96150), (']', 96150), ('不', 33677), ('人', 26960), ('一', 26076), ('风', 20763), ('无', 20361), ('山', 19765), ('来', 18069), ('花', 16535), ('有', 14894), ('春', 14411), ('日', 13537), ('天', 12645), ('中', 12414), ('何', 12012), ('时', 11631), ('云', 11563), ('是', 11191), ('年', 11012), ('知', 10967), ('水', 10841), ('自', 10792), ('得', 10730), ('上', 10641), ('月', 10454), ('如', 9884), ('生', 9397)]\n",
      "7927\n"
     ]
    }
   ],
   "source": [
    "training_chars = []\n",
    "for item in training_data:\n",
    "    training_chars += [c for c in item]\n",
    "print('We have %d words' % len(training_chars))\n",
    "\n",
    "training_chars = sorted(Counter(training_chars).items(), key=lambda x:x[1], reverse=True)\n",
    "print('We have %d different words in training set' % len(training_chars))\n",
    "print(training_chars[:30])\n",
    "\n",
    "training_chars = [c[0] for c in training_chars]\n",
    "training_char2id = {c: i + 1 for i, c in enumerate(training_chars)}\n",
    "training_id2char = {i + 1: c for i, c in enumerate(training_chars)}\n",
    "print(len(training_char2id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375 375\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "X_training_data = []\n",
    "Y_training_data = []\n",
    "\n",
    "for b in range(len(training_data) // batch_size):\n",
    "    start = b * batch_size\n",
    "    end = b * batch_size + batch_size\n",
    "    batch = [[training_char2id[c] for c in training_data[i]] for i in range(start, end)]\n",
    "    #if count==1:\n",
    "        #print(len(batch[0]))\n",
    "    maxlen = max(map(len, batch))\n",
    "    X_training_batch = np.full((batch_size, maxlen - 1), 0, np.int32)\n",
    "    Y_training_batch = np.full((batch_size, maxlen - 1), 0, np.int32)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        X_training_batch[i, :len(batch[i]) - 1] = batch[i][:-1]\n",
    "        Y_training_batch[i, :len(batch[i]) - 1] = batch[i][1:]\n",
    "    #if maxlen!=34:\n",
    "        #print(maxlen)\n",
    "    X_training_data.append(X_training_batch)\n",
    "    Y_training_data.append(Y_training_batch)\n",
    "    \n",
    "print(len(X_training_data), len(Y_training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 260536 words in validation set\n",
      "We have 5285 different words in validation set\n",
      "[('，', 16059), ('。', 16042), ('[', 8000), (']', 8000), ('不', 2780), ('人', 2207), ('一', 2109), ('风', 1753), ('无', 1665), ('山', 1654), ('来', 1422), ('花', 1401), ('有', 1259), ('春', 1246), ('日', 1098), ('何', 1049), ('天', 999), ('中', 997), ('云', 991), ('时', 960), ('年', 960), ('知', 942), ('自', 933), ('得', 919), ('是', 888), ('如', 862), ('上', 849), ('水', 846), ('月', 816), ('生', 762)]\n",
      "5285\n"
     ]
    }
   ],
   "source": [
    "validation_chars = []\n",
    "for item in validation_data:\n",
    "    validation_chars += [c for c in item]\n",
    "print('We have %d words in validation set' % len(validation_chars))\n",
    "\n",
    "validation_chars = sorted(Counter(validation_chars).items(), key=lambda x:x[1], reverse=True)\n",
    "print('We have %d different words in validation set' % len(validation_chars))\n",
    "print(validation_chars[:30])\n",
    "\n",
    "validation_chars = [c[0] for c in validation_chars]\n",
    "validation_char2id = {c: i + 1 for i, c in enumerate(validation_chars)}\n",
    "validation_id2char = {i + 1: c for i, c in enumerate(validation_chars)}\n",
    "print(len(validation_char2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 31\n",
      "30\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "X_validation_data = []\n",
    "Y_validation_data = []\n",
    "\n",
    "for b in range(len(validation_data) // batch_size):\n",
    "    start = b * batch_size\n",
    "    end = b * batch_size + batch_size\n",
    "    batch = [[validation_char2id[c] for c in validation_data[i]] for i in range(start, end)]\n",
    "    #if count==1:\n",
    "        #print(len(batch[0]))\n",
    "    maxlen = max(map(len, batch))\n",
    "    X_validation_batch = np.full((batch_size, maxlen - 1), 0, np.int32)\n",
    "    Y_validation_batch = np.full((batch_size, maxlen - 1), 0, np.int32)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        X_validation_batch[i, :len(batch[i]) - 1] = batch[i][:-1]\n",
    "        Y_validation_batch[i, :len(batch[i]) - 1] = batch[i][1:]\n",
    "    #if maxlen!=34:\n",
    "        #print(maxlen)\n",
    "    X_validation_data.append(X_validation_batch)\n",
    "    Y_validation_data.append(Y_validation_batch)\n",
    "    \n",
    "print(len(X_validation_data), len(Y_validation_data))\n",
    "print(b)\n",
    "print(type(X_validation_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('dictionary.pkl', 'wb') as fw:\n",
    "    pickle.dump([training_char2id, training_id2char], fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "hidden_size = 256\n",
    "num_layer = 2\n",
    "embedding_size = 256\n",
    "\n",
    "X_training= tf.placeholder(tf.int32, [batch_size, None])\n",
    "Y_training= tf.placeholder(tf.int32, [batch_size, None])\n",
    "'''\n",
    "X_validation= tf.placeholder(tf.int32, [batch_size, None])\n",
    "Y_validation= tf.placeholder(tf.int32, [batch_size, None])\n",
    "'''\n",
    "#print(X)\n",
    "learning_rate = tf.Variable(0.0, trainable=False)\n",
    "#print(learning_rate)\n",
    "ikeep_prob = tf.placeholder(tf.float32, name='ikeep_prob')\n",
    "okeep_prob = tf.placeholder(tf.float32, name='okeep_prob')\n",
    "cell = tf.nn.rnn_cell.MultiRNNCell(\n",
    "    [tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(hidden_size), input_keep_prob=ikeep_prob, output_keep_prob=ikeep_prob )for i in range(num_layer)], \n",
    "    state_is_tuple=True)\n",
    "\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "tr_embeddings = tf.Variable(tf.random_uniform([len(training_char2id) + 1, embedding_size], -1.0, 1.0))\n",
    "tr_embedded = tf.nn.embedding_lookup(tr_embeddings, X_training)\n",
    "#print(tr_embeddings)\n",
    "# outputs: batch_size, max_time, hidden_size\n",
    "# last_states: 2 tuple(two LSTM), 2 tuple(c and h)\n",
    "#              batch_size, hidden_size\n",
    "tr_outputs, tr_last_states = tf.nn.dynamic_rnn(cell, tr_embedded, initial_state=initial_state)\n",
    "tr_outputs = tf.reshape(tr_outputs, [-1, hidden_size])                # batch_size * max_time, hidden_size\n",
    "\n",
    "training_logits = tf.layers.dense(tr_outputs, units=len(training_char2id) + 1)       # batch_size * max_time, len(char2id) + 1\n",
    "training_logits = tf.reshape(training_logits, [batch_size, -1, len(training_char2id) + 1]) # batch_size, max_time, len(char2id) + 1\n",
    "probs = tf.nn.softmax(training_logits)                                   # batch_size, max_time, len(char2id) + 1\n",
    "loss = tf.reduce_mean(tf.contrib.seq2seq.sequence_loss(training_logits, Y_training, tf.ones_like(Y_training, dtype=tf.float32)))\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "val_embeddings = tf.Variable(tf.random_uniform([len(training_char2id) + 1, embedding_size], -1.0, 1.0))\n",
    "val_embedded = tf.nn.embedding_lookup(val_embeddings, X_validation)\n",
    "print(val_embeddings)\n",
    "# outputs: batch_size, max_time, hidden_size\n",
    "# last_states: 2 tuple(two LSTM), 2 tuple(c and h)\n",
    "#              batch_size, hidden_size\n",
    "val_outputs, val_last_states = tf.nn.dynamic_rnn(cell, val_embedded, initial_state=initial_state)\n",
    "val_outputs = tf.reshape(val_outputs, [-1, hidden_size])                # batch_size * max_time, hidden_size\n",
    "validation_logits = tf.layers.dense(val_outputs, units=len(training_char2id) + 1)       # batch_size * max_time, len(char2id) + 1\n",
    "validation_logits = tf.reshape(validation_logits, [batch_size, -1, len(training_char2id) + 1]) # batch_size, max_time, len(char2id) + 1\n",
    "'''\n",
    "\n",
    "#validation_accuracy=tf.reduce_mean(tf.contrib.seq2seq.sequence_loss(training_logits, Y_training, tf.ones_like(Y_training, dtype=tf.float32)))\n",
    "\n",
    "\n",
    "\n",
    "params = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, params), 5)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).apply_gradients(zip(grads, params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x00000259685E0BA8>\n"
     ]
    }
   ],
   "source": [
    "print(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(256, ?), dtype=int32)\n",
      "<tf.Variable 'Variable_1:0' shape=(7928, 256) dtype=float32_ref>\n",
      "Tensor(\"embedding_lookup/Identity:0\", shape=(256, ?, 256), dtype=float32)\n",
      "Tensor(\"Reshape:0\", shape=(?, 256), dtype=float32)\n",
      "Tensor(\"Reshape_1:0\", shape=(256, ?, 7928), dtype=float32)\n",
      "(LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_3:0' shape=(256, 256) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_4:0' shape=(256, 256) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'rnn/while/Exit_5:0' shape=(256, 256) dtype=float32>, h=<tf.Tensor 'rnn/while/Exit_6:0' shape=(256, 256) dtype=float32>))\n",
      "[<tf.Variable 'Variable_1:0' shape=(7928, 256) dtype=float32_ref>, <tf.Variable 'rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0' shape=(512, 1024) dtype=float32_ref>, <tf.Variable 'rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0' shape=(1024,) dtype=float32_ref>, <tf.Variable 'rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0' shape=(512, 1024) dtype=float32_ref>, <tf.Variable 'rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0' shape=(1024,) dtype=float32_ref>, <tf.Variable 'dense/kernel:0' shape=(256, 7928) dtype=float32_ref>, <tf.Variable 'dense/bias:0' shape=(7928,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "print(X_training)\n",
    "\n",
    "print(tr_embeddings)\n",
    "print(tr_embedded)\n",
    "print(tr_outputs)\n",
    "print(training_logits)\n",
    "print(tr_last_states)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 375/375 [01:07<00:00,  5.52it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:02<00:00, 11.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 6.69854\n",
      "Epoch 0 Validates 6.52082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 375/375 [01:06<00:00,  5.74it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:02<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 6.32235\n",
      "Epoch 1 Validates 6.13153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 375/375 [01:06<00:00,  5.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:02<00:00, 11.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss 6.01712\n",
      "Epoch 2 Validates 6.06950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 375/375 [01:06<00:00,  5.36it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:02<00:00, 11.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss 5.89121\n",
      "Epoch 3 Validates 6.10205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 375/375 [01:06<00:00,  5.58it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:02<00:00, 11.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss 5.78708\n",
      "Epoch 4 Validates 6.14971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 375/375 [01:06<00:00,  5.68it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:02<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss 5.65762\n",
      "Epoch 5 Validates 6.23994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 375/375 [01:06<00:00,  5.48it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:02<00:00, 11.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss 5.50609\n",
      "Epoch 6 Validates 6.35740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 375/375 [01:06<00:00,  5.37it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:02<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss 5.41112\n",
      "Epoch 7 Validates 6.43625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 375/375 [01:06<00:00,  5.84it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 31/31 [00:02<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss 5.34969\n",
      "Epoch 8 Validates 6.48057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▉                                                                         | 37/375 [00:06<00:53,  6.33it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-fcae7ae7550f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_training_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mls_\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX_training\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_training_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_training\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mY_training_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mikeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mokeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mls_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\Tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "Loss=[]\n",
    "Validation=[]\n",
    "for epoch in range(100):\n",
    "    sess.run(tf.assign(learning_rate, 0.0004 * (0.97 ** epoch)))\n",
    "    \n",
    "    data_index = np.arange(len(X_training_data))\n",
    "    np.random.shuffle(data_index)\n",
    "    X_training_data = [X_training_data[i] for i in data_index]\n",
    "    Y_training_data = [Y_training_data[i] for i in data_index]\n",
    "    \n",
    "    losses = []\n",
    "    validates=[]\n",
    "\n",
    "    for i in tqdm(range(len(X_training_data))):\n",
    "        ls_,  _ = sess.run([loss, optimizer], feed_dict={X_training: X_training_data[i], Y_training: Y_training_data[i], ikeep_prob: 0.8, okeep_prob: 0.8})\n",
    "        losses.append(ls_)\n",
    "    #\n",
    "    \n",
    "    for i in tqdm(range(len(X_validation_data))):\n",
    "        validate_acc = sess.run([loss], feed_dict={X_training: X_validation_data[i], Y_training: Y_validation_data[i], ikeep_prob: 1, okeep_prob: 1})\n",
    "        validates.append(validate_acc)\n",
    "    '''\n",
    "    #\n",
    "    \n",
    "    if epoch+1==10:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, './10/10_epoch')\n",
    "    \n",
    "    if epoch+1==20:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, './20/20_epoch')\n",
    "    if epoch+1==30:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, './30/30_epoch')\n",
    "    if epoch+1==40:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, './40/40_epoch')\n",
    "    if epoch+1==50:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, './50/50_epoch')\n",
    "        \n",
    "    if epoch+1==60:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, './60/60_epoch')\n",
    "    \n",
    "    if epoch+1==70:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, './70/70_epoch')\n",
    "    if epoch+1==80:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, './80/80_epoch')\n",
    "    if epoch+1==90:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, './90/90_epoch')\n",
    "    if epoch+1==100:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, './100/100_epoch')\n",
    "    '''\n",
    "    \n",
    "    print('Epoch %d Loss %.5f' % (epoch, np.mean(losses)))\n",
    "    Loss.append(np.mean(losses))\n",
    "    print('Epoch %d Validates %.5f' % (epoch, np.mean(validates)))\n",
    "    Validation.append(np.mean(validates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.708487, 6.542999, 6.47859, 6.1634636, 5.9582043, 5.8722515, 5.8110027, 5.701758, 5.6210666, 5.580378, 5.5502963, 5.5204234, 5.4796486, 5.4413443, 5.4097214, 5.3864746, 5.366487, 5.3481503, 5.327319, 5.3054676, 5.281816, 5.2583537, 5.236086, 5.2168775, 5.198056, 5.1808333, 5.1650047, 5.1502104, 5.135682, 5.12194, 5.109127, 5.096859, 5.0848994, 5.0738664, 5.0636315, 5.0533843, 5.0444274, 5.0353065, 5.0265217, 5.0185204, 5.011227, 5.0037847, 4.9969974, 4.990392, 4.983714, 4.977443, 4.972288, 4.9653583, 4.960594, 4.9556413]\n",
      "[6.5855975, 6.530785, 6.3774776, 6.056278, 6.0460277, 6.0778613, 6.1174946, 6.2397423, 6.2987475, 6.3240294, 6.313289, 6.3476143, 6.3538804, 6.377427, 6.4043336, 6.426238, 6.4298086, 6.44656, 6.44417, 6.459502, 6.461572, 6.480503, 6.502726, 6.5101876, 6.520776, 6.5377436, 6.546568, 6.556171, 6.5809355, 6.5737624, 6.5882535, 6.5878105, 6.6167054, 6.6074634, 6.621916, 6.6341963, 6.625648, 6.630437, 6.6416764, 6.65174, 6.653723, 6.6700177, 6.656472, 6.668778, 6.6718335, 6.678764, 6.677299, 6.6880054, 6.6887035, 6.7018495]\n"
     ]
    }
   ],
   "source": [
    "print(Loss)\n",
    "print(Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
